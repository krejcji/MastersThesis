\chapter{Experimental results and discussion}
\section{Methodology}

\subsection{Algorithms}

\subsection{Datasets and models}

\subsection{Evaluation Metric}

\subsection{Experimental Setup}

% I could split this chapter into two.

\section{Real benchmarks}
\subsection{CIFAR10 - demo}

Probably the most widely used image classification dataset is the CIFAR-10. We use it to evaluate the performance of Random Search, DEHB, Optuna and DyHPO hyperparameter optimization algorithms \xxx{(not the final set of algorithms)}. The optimization was rerun 5 times for each algorithm with different seed \xxx{but this seems to be too little, the means are not significantly different}. The optimization problem consisted of six hyperparameters, three float and three integer. We used AdamW optimizer. Learning rate and the final value of cosine decay $\eta_{min}$ were optimized. The neural network architecture consists of sequential convolutional layers with batch normalization. We optimize the number of convolutional layers and the number of filters. Then the fully connected layer follows and we optimize its size. The hyperparameters and their domains are summarized in the Table~\ref{tc10}.

\begin{table}[H]
    \centering
    \begin{tabular}{cc}
        \textbf{Hyperparameter} & \textbf{Values} \\ \midrule
        Learning rate & $\{1\mathrm{e}{-4}, 1\mathrm{e}{-1}\}$ \\
        $\eta_{min}$ & $\{1\mathrm{e}{-5}, 0.99\}$ \\
        Dropout & $\{0.0, 1.0\}$ \\
        FC neurons & $\{8, 128\}$ \\
        Channels multiplier & $\{1, 8\}$ \\
        Conv layers & $\{1, 4\}$ \\
    \end{tabular}
    \caption{CIFAR-10 HPO optimization search space.}
    \label{tc10}
\end{table}

The network is trained for up to 70 epochs. The budget for the hyperparameter optimization is 17 full evaluations, which is equal to 1190 epochs. We compare the algorithms in classification accuracy on the validation set. We use the trial number on the x-axis, which neglects the cost of the hyperparameter optimization algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.64]{./img/cifar_exp_plot.pdf}
    \caption{CIFAR-10 comparison of the best cumulative accuracy, 17 full evaluations (1190 epochs)}
    %\label{}
\end{figure}

The following plots are evaluated after 350 training epochs \xxx{because that is where the difference is the largest; so that we can test if the results will be statistically significant, or if we need more repetitions}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.74]{./img/cifar_exp_boxplot.pdf}
    \caption{CIFAR-10 boxplot after 350 epochs.}
    %\label{}
\end{figure}

%\xxx{I wanted to use critical difference diagrams for the visualization of performance and statistical analysis because I think they are intuitive and easily readable, but I'm not sure if I can do it even for unpaired experiements. CD diagrams rely on computing the rank of the methods --- comparing the results of different runs to each other.} We used the Mann-Whitney rank sum test for the post-hoc analysis of the results with the significance level $\alpha=0.05$ adjusted with the Holm's method.

% \begin{figure}[H]
%     \centering
%     \includegraphics[scale=0.74]{./img/cifar_exp_cd_ranks.pdf}
%     \caption{CIFAR-10 critical difference diagram}
%     %\label{}
% \end{figure}
\subsection{SVHN}


\section{Discussion}

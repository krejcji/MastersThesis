% Surveys
@article{survey2020,
  title={On hyperparameter optimization of machine learning algorithms: Theory and practice},
  author={Yang, Li and Shami, Abdallah},
  journal={Neurocomputing},
  volume={415},
  pages={295--316},
  year={2020},
  publisher={Elsevier}
}

@article{survey2021,
  title={Hyperparameter optimization: Foundations, algorithms, best practices and open challenges. arXiv 2021},
  author={Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and others},
  journal={arXiv preprint arXiv:2107.05847}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}


% Practical guides
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}


% Hyperparameter optimization frameworks
@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@inproceedings{akiba2019optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@article{smac3,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}

@inproceedings{hutter2011sequential,
  title={Sequential model-based optimization for general algorithm configuration},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5},
  pages={507--523},
  year={2011},
  organization={Springer}
}

@article{hutter2010sequential,
  title={Sequential model-based optimization for general algorithm configuration (extended version)},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  journal={Technical Report TR-2010--10, University of British Columbia, Computer Science, Tech. Rep.},
  year={2010}
}


% Multi-fidelity optimization
@article{swersky2014freeze,
  title={Freeze-thaw Bayesian optimization},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}

@inproceedings{domhan2015speeding,
  title={Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author={Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@inproceedings{klein2017fast,
  title={Fast bayesian optimization of machine learning hyperparameters on large datasets},
  author={Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  booktitle={Artificial intelligence and statistics},
  pages={528--536},
  year={2017},
  organization={PMLR}
}

@article{li2018hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={185},
  pages={1--52},
  year={2018}
}

@inproceedings{falkner2018bohb,
  title={BOHB: Robust and efficient hyperparameter optimization at scale},
  author={Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle={International conference on machine learning},
  pages={1437--1446},
  year={2018},
  organization={PMLR}
}

@article{awad2021dehb,
  title={Dehb: Evolutionary hyperband for scalable, robust and efficient hyperparameter optimization},
  author={Awad, Noor and Mallik, Neeratyoy and Hutter, Frank},
  journal={arXiv preprint arXiv:2105.09821},
  year={2021}
}

@inproceedings{zhu2020accelerating,
  title={Accelerating hyperparameter optimization of deep neural network via progressive multi-fidelity evaluation},
  author={Zhu, Guanghui and Zhu, Ruancheng},
  booktitle={Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11--14, 2020, Proceedings, Part I 24},
  pages={752--763},
  year={2020},
  organization={Springer}
}

@article{zimmer2021auto,
  title={Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl},
  author={Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={9},
  pages={3079--3090},
  year={2021},
  publisher={IEEE}
}

@article{wistuba2022supervising,
  title={Supervising the multi-fidelity race of hyperparameter configurations},
  author={Wistuba, Martin and Kadra, Arlind and Grabocka, Josif},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13470--13484},
  year={2022}
}

@article{yu2024two,
  title={Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset},
  author={Yu, Sungduk and Ma, Po-Lun and Singh, Balwinder and Silva, Sam and Pritchard, Mike},
  journal={Artificial Intelligence for the Earth Systems},
  volume={3},
  number={1},
  pages={e230013},
  year={2024},
  publisher={American Meteorological Society}
}


% Transfer learning
@inproceedings{bardenet2013collaborative,
  title={Collaborative hyperparameter tuning},
  author={Bardenet, R{\'e}mi and Brendel, M{\'a}ty{\'a}s and K{\'e}gl, Bal{\'a}zs and Sebag, Michele},
  booktitle={International conference on machine learning},
  pages={199--207},
  year={2013},
  organization={PMLR}
}

@inproceedings{yogatama2014efficient,
  title={Efficient transfer learning method for automatic hyperparameter tuning},
  author={Yogatama, Dani and Mann, Gideon},
  booktitle={Artificial intelligence and statistics},
  pages={1077--1085},
  year={2014},
  organization={PMLR}
}

@article{perrone2018scalable,
  title={Scalable hyperparameter transfer learning},
  author={Perrone, Valerio and Jenatton, Rodolphe and Seeger, Matthias W and Archambeau, C{\'e}dric},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{wang2021pre,
  title={Pre-trained Gaussian processes for Bayesian optimization},
  author={Wang, Zi and Dahl, George E and Swersky, Kevin and Lee, Chansoo and Nado, Zachary and Gilmer, Justin and Snoek, Jasper and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:2109.08215},
  year={2021}
}

% Bayesian optimization
@inproceedings{mockus1974bayesian,
  title={On Bayesian methods for seeking the extremum},
  author={Mockus, Jonas},
  booktitle={Proceedings of the IFIP Technical Conference},
  pages={400--404},
  year={1974}
}

 % Expected improvement
@article{jones1998efficient,
  title={Efficient global optimization of expensive black-box functions},
  author={Jones, Donald R and Schonlau, Matthias and Welch, William J},
  journal={Journal of Global optimization},
  volume={13},
  pages={455--492},
  year={1998},
  publisher={Springer}
}

 % Probability of improvement
@article{kushner1964new,
  title={A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise},
  author={Kushner, Harold J},
  year={1964}
}

@book{rasmussen2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{wang2023intuitive,
  title={An intuitive tutorial to Gaussian processes regression},
  author={Wang, Jie},
  journal={Computing in Science \& Engineering},
  year={2023},
  publisher={IEEE}
}

@article{brochu2010tutorial,
  title={A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}

@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@article{watanabe2023tree,
  title={Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance},
  author={Watanabe, Shuhei},
  journal={arXiv preprint arXiv:2304.11127},
  year={2023}
}

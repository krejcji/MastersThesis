@article{survey2020,
  title={On hyperparameter optimization of machine learning algorithms: Theory and practice},
  author={Yang, Li and Shami, Abdallah},
  journal={Neurocomputing},
  volume={415},
  pages={295--316},
  year={2020},
  publisher={Elsevier}
}

@article{survey2021,
  title={Hyperparameter optimization: Foundations, algorithms, best practices and open challenges. arXiv 2021},
  author={Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and others},
  journal={arXiv preprint arXiv:2107.05847}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}

@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}

@inproceedings{akiba2019optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@article{smac3,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}

@article{li2018hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={185},
  pages={1--52},
  year={2018}
}

@article{wistuba2022supervising,
  title={Supervising the multi-fidelity race of hyperparameter configurations},
  author={Wistuba, Martin and Kadra, Arlind and Grabocka, Josif},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13470--13484},
  year={2022}
}

@inproceedings{bardenet2013collaborative,
  title={Collaborative hyperparameter tuning},
  author={Bardenet, R{\'e}mi and Brendel, M{\'a}ty{\'a}s and K{\'e}gl, Bal{\'a}zs and Sebag, Michele},
  booktitle={International conference on machine learning},
  pages={199--207},
  year={2013},
  organization={PMLR}
}

@inproceedings{yogatama2014efficient,
  title={Efficient transfer learning method for automatic hyperparameter tuning},
  author={Yogatama, Dani and Mann, Gideon},
  booktitle={Artificial intelligence and statistics},
  pages={1077--1085},
  year={2014},
  organization={PMLR}
}

@article{perrone2018scalable,
  title={Scalable hyperparameter transfer learning},
  author={Perrone, Valerio and Jenatton, Rodolphe and Seeger, Matthias W and Archambeau, C{\'e}dric},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{wang2021pre,
  title={Pre-trained Gaussian processes for Bayesian optimization},
  author={Wang, Zi and Dahl, George E and Swersky, Kevin and Lee, Chansoo and Nado, Zachary and Gilmer, Justin and Snoek, Jasper and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:2109.08215},
  year={2021}
}

@inproceedings{domhan2015speeding,
  title={Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author={Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@article{swersky2014freeze,
  title={Freeze-thaw Bayesian optimization},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}

% Bayesian optimization
@article{brochu2010tutorial,
  title={A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}

@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

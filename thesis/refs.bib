% Universal approximation theorem
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

% Surveys
@article{survey2020,
  title={On hyperparameter optimization of machine learning algorithms: Theory and practice},
  author={Yang, Li and Shami, Abdallah},
  journal={Neurocomputing},
  volume={415},
  pages={295--316},
  year={2020},
  publisher={Elsevier}
}

@article{survey2021,
  title={Hyperparameter optimization: Foundations, algorithms, best practices and open challenges. arXiv 2021},
  author={Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and others},
  journal={arXiv preprint arXiv:2107.05847}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization.},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}


% Practical guides
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}


% Hyperparameter optimization frameworks
@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@inproceedings{akiba2019optuna,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

@article{smac3,
  author  = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and André Biedenkapp and Difan Deng and Carolin Benjamins and Tim Ruhkopf and René Sass and Frank Hutter},
  title   = {SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {54},
  pages   = {1--9},
  url     = {http://jmlr.org/papers/v23/21-0888.html}
}

@inproceedings{hutter2011sequential,
  title={Sequential model-based optimization for general algorithm configuration},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5},
  pages={507--523},
  year={2011},
  organization={Springer}
}

@article{hutter2010sequential,
  title={Sequential model-based optimization for general algorithm configuration (extended version)},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  journal={Technical Report TR-2010--10, University of British Columbia, Computer Science, Tech. Rep.},
  year={2010}
}


% Multi-fidelity optimization
@article{swersky2014freeze,
  title={Freeze-thaw Bayesian optimization},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}

@inproceedings{domhan2015speeding,
  title={Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author={Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@InProceedings{jamieson16,
  title = 	 {Non-stochastic Best Arm Identification and Hyperparameter Optimization},
  author = 	 {Jamieson, Kevin and Talwalkar, Ameet},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {240--248},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/jamieson16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/jamieson16.html},
  abstract = 	 {Motivated by the task of hyperparameter optimization, we introduce the \em non-stochastic best-arm identification problem. We identify an attractive algorithm  for this setting that makes no assumptions on the convergence behavior of the arms’ losses, has no free-parameters to adjust, provably outperforms the uniform allocation baseline in favorable conditions, and performs comparably (up to \log factors) otherwise. Next, by leveraging the iterative nature of many  learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification. Our empirical results show that, by allocating more resources to promising hyperparameter settings, our approach achieves comparable test accuracies an order of magnitude faster than the uniform strategy. The robustness and simplicity of our approach makes it well-suited to ultimately replace the uniform strategy currently used in most machine learning software packages.}
}

@inproceedings{klein2017fast,
  title={Fast bayesian optimization of machine learning hyperparameters on large datasets},
  author={Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  booktitle={Artificial intelligence and statistics},
  pages={528--536},
  year={2017},
  organization={PMLR}
}

@article{li2018hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={185},
  pages={1--52},
  year={2018}
}

@inproceedings{falkner2018bohb,
  title={BOHB: Robust and efficient hyperparameter optimization at scale},
  author={Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle={International conference on machine learning},
  pages={1437--1446},
  year={2018},
  organization={PMLR}
}

@article{klein2020model,
  title={Model-based asynchronous hyperparameter and neural architecture search},
  author={Klein, Aaron and Tiao, Louis C and Lienart, Thibaut and Archambeau, Cedric and Seeger, Matthias},
  journal={arXiv preprint arXiv:2003.10865},
  year={2020}
}

@article{li2020system,
  title={A system for massively parallel hyperparameter tuning},
  author={Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Ben-Tzur, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={230--246},
  year={2020}
}

@article{awad2021dehb,
  title={Dehb: Evolutionary hyperband for scalable, robust and efficient hyperparameter optimization},
  author={Awad, Noor and Mallik, Neeratyoy and Hutter, Frank},
  journal={arXiv preprint arXiv:2105.09821},
  year={2021}
}

@inproceedings{zhu2020accelerating,
  title={Accelerating hyperparameter optimization of deep neural network via progressive multi-fidelity evaluation},
  author={Zhu, Guanghui and Zhu, Ruancheng},
  booktitle={Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11--14, 2020, Proceedings, Part I 24},
  pages={752--763},
  year={2020},
  organization={Springer}
}

@article{zimmer2021auto,
  title={Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl},
  author={Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={9},
  pages={3079--3090},
  year={2021},
  publisher={IEEE}
}

@article{li2022hyper,
  title={Hyper-tune: Towards efficient hyper-parameter tuning at scale},
  author={Li, Yang and Shen, Yu and Jiang, Huaijun and Zhang, Wentao and Li, Jixiang and Liu, Ji and Zhang, Ce and Cui, Bin},
  journal={arXiv preprint arXiv:2201.06834},
  year={2022}
}

@article{wistuba2022supervising,
  title={Supervising the multi-fidelity race of hyperparameter configurations},
  author={Wistuba, Martin and Kadra, Arlind and Grabocka, Josif},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13470--13484},
  year={2022}
}

@article{yu2024two,
  title={Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset},
  author={Yu, Sungduk and Ma, Po-Lun and Singh, Balwinder and Silva, Sam and Pritchard, Mike},
  journal={Artificial Intelligence for the Earth Systems},
  volume={3},
  number={1},
  pages={e230013},
  year={2024},
  publisher={American Meteorological Society}
}


% Transfer learning
@inproceedings{bardenet2013collaborative,
  title={Collaborative hyperparameter tuning},
  author={Bardenet, R{\'e}mi and Brendel, M{\'a}ty{\'a}s and K{\'e}gl, Bal{\'a}zs and Sebag, Michele},
  booktitle={International conference on machine learning},
  pages={199--207},
  year={2013},
  organization={PMLR}
}

@inproceedings{yogatama2014efficient,
  title={Efficient transfer learning method for automatic hyperparameter tuning},
  author={Yogatama, Dani and Mann, Gideon},
  booktitle={Artificial intelligence and statistics},
  pages={1077--1085},
  year={2014},
  organization={PMLR}
}

@article{perrone2018scalable,
  title={Scalable hyperparameter transfer learning},
  author={Perrone, Valerio and Jenatton, Rodolphe and Seeger, Matthias W and Archambeau, C{\'e}dric},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{wang2021pre,
  title={Pre-trained Gaussian processes for Bayesian optimization},
  author={Wang, Zi and Dahl, George E and Swersky, Kevin and Lee, Chansoo and Nado, Zachary and Gilmer, Justin and Snoek, Jasper and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:2109.08215},
  year={2021}
}

% Bayesian optimization
@inproceedings{mockus1974bayesian,
  title={On Bayesian methods for seeking the extremum},
  author={Mockus, Jonas},
  booktitle={Proceedings of the IFIP Technical Conference},
  pages={400--404},
  year={1974}
}

 % Expected improvement
@article{jones1998efficient,
  title={Efficient global optimization of expensive black-box functions},
  author={Jones, Donald R and Schonlau, Matthias and Welch, William J},
  journal={Journal of Global optimization},
  volume={13},
  pages={455--492},
  year={1998},
  publisher={Springer}
}

 % Probability of improvement
@article{kushner1964new,
  title={A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise},
  author={Kushner, Harold J},
  year={1964}
}

@book{rasmussen2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT press Cambridge, MA}
}

@article{wang2023intuitive,
  title={An intuitive tutorial to Gaussian processes regression},
  author={Wang, Jie},
  journal={Computing in Science \& Engineering},
  year={2023},
  publisher={IEEE}
}

@article{brochu2010tutorial,
  title={A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning},
  author={Brochu, Eric and Cora, Vlad M and De Freitas, Nando},
  journal={arXiv preprint arXiv:1012.2599},
  year={2010}
}

@article{bergstra2011algorithms,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@article{watanabe2023tree,
  title={Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance},
  author={Watanabe, Shuhei},
  journal={arXiv preprint arXiv:2304.11127},
  year={2023}
}
